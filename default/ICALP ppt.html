<h2>Slide 1</h2>
<p>Hello, every one.<br>
I’m gonna talk about the paper “Improved Time …”</p>
<h2>Slide 2</h2>
<p>To introduce the problem, we first define the term “non-decreasing path”.<br>
As the name implies, a non-decreasing path is “a path on which  …”<br>
Now, our goal is to compute the minimum last edge weight of any NDP between all pairs of vertices.<br>
Formally, we want to compute a matrix R, whose entry (i,j) equals “the minimum last edge weight …”    (or \infty if there is no such a path)<br>
As usual, we’ll use m and n to denote the number of edges and the number of vertices respectively.</p>
<blockquote>
<p>use w§ to denote the minimum last edge weight of P<br>
If consider the weight to be time, then we want to travel from i to j with the earliest arrival time.</p>
</blockquote>
<h2>Slide 3</h2>
<p>Ok, here are some related results.<br>
The most related problem is “Single Source …” which asks to compute the same thing, but only for a single given source.<br>
A folklore modification of the Dijkstra’s algorithms solves it in “” time<br>
Then, about a decade ago, this paper by Virginia gave a  O(m log log n) time  algorithm, which is faster in sparse graph. Besides, this paper also gave a linear time algorithm, but in the word-RAM model of computation.</p>
<p>For the APNP problem, the same paper gave “the first …”, which runs in this time, with the exponent (15+w)/6.  This result can be directly improved by using a faster sub-routine. (Where \omega is the exponent of the time for matrix multiplication)</p>
<p>Note that the APNP problem is “at least as hard as …”<br>
In this paper, we first give a result of “”,  and then slightly improve it to “”.<br>
The relation of these values can be seen in this graph.</p>
<h2>Slide 4</h2>
<p>In this paper, we first define two matrix products, which are kind of similar to the standard matrix multiplication.<br>
The first is the dominance product, which counts the number of dominated entries.<br>
Another is the “star” product, which is similar as dominance product, but gives the minimum dominating entry in matrix B.<br>
The best known result for the star product is given by this paper with this result.</p>
<blockquote>
<p>This is the faster subroutine that improves virginia’s APNP algorithm</p>
</blockquote>
<h2>Slide 5</h2>
<p>Suppose A is the weight matrix of graph G. It can be seen as the APNP result matrix for restricted NDPs containing exactly one edge of G<br>
Then A\star A corresponds to NDPs containing …<br>
And the minimum of them corresponds to …<br>
In this example, the entry (1,3) of A\star A get the value 2, which you can see from the graph: there is path from vertex 1 to 2 and then to 3.</p>
<h2>Slide 6</h2>
<p>Let’s call it the Min-Star operation, which functions as extending one edge.<br>
If R corresponds to NDPs …<br>
Then the result of the Min-Star operation corresponds to NDPs …<br>
So, we can come up a Naive algorithm like this. It just repeats the Min-Star operation (n-1) times.<br>
However, it costs more than O(n^3) time.</p>
<h2>Slide 7</h2>
<p>Here we give a different perspective of the Min-Star operation<br>
It’s easy to see that in the star product, the entries in k-th column of R are only compared to the entries in k-th row of A<br>
So, we can compute the Min-Star operation in a “Scanning” style. Specifically, we use every entry of R to scan its corresponding row in A.  It seems that this is even worse since it costs O(n^3) time for each Min-Star operation, which results in O(n^4) time in total.<br>
But if we look a little closely, we can see that in the “…”<br>
So, if we sort the list of finite entries of each row of A, then we can eliminate “…”</p>
<h2>Slide 8,9,10</h2>
<p>Here is a simple example.<br>
We use a matrix N to record the position of scanning. N[i,j] is initialized to the beginning position.<br>
Here we use R’ to scan A once and the result matrix is R.<br>
We use every finite entry of R’ to scan the corresponding row of A from the position N[i,k]. Whenever an entry is scanned, update R as needed.<br>
In this scanning, …</p>
<p>Here, we use the scanning entry of value 7 to scan the first row of A<br>
When the first entry 12 is scanned, the corresponding entry in R is updated.<br>
The the same for the second scanned entry 8<br>
We continue scanning until “…”<br>
The key point is that: the next repetition of scanning begins …<br>
So,  for each entry (i,j), the number of scanned entries in the whole n-1 repetitions of scanning is at most n.<br>
Therefore, the total time for n-1 repetition of the Min-star operation is reduced to O(n^3)</p>
<h2>Slide 11</h2>
<p>It’s time to introduce our algorithms.<br>
We first divide “…”<br>
So each sub-matrix contains …<br>
We claim that any NDP is … because of the nature of NDP<br>
With only one exception: there may be some edges …<br>
We handle this by splitting out those edges into …<br>
There are at most L such matrices<br>
Then we can extend NDPs from A_l to A’_l by matrix multiplication .<br>
Because the weights in A’_l are same, which is w_l, and which is greater than all finite entries in A_l<br>
Ok, from now on, we’ll ignore this issue.</p>
<h2>Slide 12</h2>
<p>Here are some useful notations.<br>
Note that n_l represents the number of …<br>
Note that “new” entries are those entries become finite in R_l<br>
We conclude that the sum of all n_l’s is at most n^2 because …</p>
<h2>Slide 13</h2>
<p>The main idea of out algorithm is to compute R_1, R_2, …, R_L one by one<br>
At the beginning of each phase, we first perform “one edge extension” by …<br>
After this, we get a matrix R_l’, which represents the APNP matrix …<br>
Then, we do scanning at most (n-1) times<br>
You may ask why we do this two steps, rather than just scan n-1 times.<br>
Actually, this is one of the main ideas of our paper.</p>
<h2>Slide 14</h2>
<p>After one edge extension, we only need to use those “new entries to do scanning” because …<br>
So our algorithm works like this</p>
<h2>Slide 15</h2>
<p>Generally, the  … as mentioned before<br>
But there are two observations: first …<br>
and second A_l contains … it’s sparse<br>
By taking advantage of them, the one edge extension can be computed …<br>
First, We use boolean matrix multiplication to …<br>
Then we use column-balancing …</p>
<h2>Slide 16</h2>
<p>Ok, now we give a simple case.<br>
Under the assumption that each row of A_l … which means that … recall that each G_l contains at most n^2/L<br>
Then the scanning time is …<br>
Hence the total time is beautiful. It’s optimum in some sense.<br>
So, the question is: What if … ?</p>
<h2>Slide 17</h2>
<p>To solve this question, we use a similar method as the APNP algorithm in Virginia’s paper<br>
The algorithm in that paper first compute …<br>
Then it samples a hitting set S of this amount of vertices and compute all NDPs …</p>
<p>The algorithm works because: If an …<br>
So we can compute p[i,j] by concatenating two subpaths.<br>
One is from i to some vertex in S, whose information can be read directly from R<br>
Another subpath is from this vertex in S to j, which needs the following oracle</p>
<h2>Slide 18</h2>
<p>The oracle T(h_1,j) for …<br>
The oracle for each h_1 can be …<br>
There is a byproduct matrix H_2 which …<br>
This matrix will play an important role in our algorithm</p>
<h2>Slide 19</h2>
<p>Here is an example.<br>
Note that when w’=2, w’’=4 because …<br>
w’’ can be found by binary searching<br>
For example, given 1, it outputs 4<br>
given 4, it outputs 7<br>
The size of set S is … and the number of queries is … since we query for each pair i,j and each vertex in S<br>
The total time of Virginia’s algorithm is hence this value</p>
<h2>Slide 20</h2>
<p>Now let’s get back to our algorithms.<br>
What remains is the question …<br>
We handle it using the concatenation method as before.<br>
For ease of analysis, we replace the notation L with n^t<br>
Then we need to deal with the case when “rows of A_l …”<br>
We distinguish two kinds of vertices: …<br>
Then we solve the question by these two steps:<br>
first scan … , after this, we have examined all NDPs that do not pass any high vertex<br>
then examine …</p>
<h2>Slide 21</h2>
<p>We use the same oracle for concatenation<br>
The time for computing the oracle is …<br>
Each query costs<br>
However, a new problem comes up.<br>
We can’t afford so many queries<br>
Here comes the another main idea of our paper.</p>
<h2>Slide 22</h2>
<p>Fortunately, we can query only …<br>
We first split a matrix H_1 from R^L where R^L is …<br>
Recall that we’ve got a byproduct when computing the oracle<br>
By computing a boolean matrix multiplication<br>
We claim that we only need to query …<br>
It takes this time by using sparse …<br>
Specifically, the query procedure works like this<br>
The number of queries is bounded by this value</p>
<h2>Slide 23</h2>
<p>Here is the main framework of our algorithm<br>
Each phase consists of three steps: …</p>
<h2>Slide 24</h2>
<p>Here is the total time</p>
<h2>Slide 25</h2>
<p>Now we concatenate n^q+1 paths<br>
There are two kinds of paths</p>
<h2>Slide 26</h2>
<p>In each phase, we repeat n^q times of scanning and concatenating<br>
Then we’ve computed paths through at most …</p>
<h2>Slide 27</h2>
<p>The time for the star product can be reduced because they are two rectangular matrix. In this time, \omega(1,k,1) is …<br>
Also, note that the scanning time …<br>
For paths through more than n^q high vertices, …</p>
<h2>Slide 28</h2>
<p>By reducing rectangular matrix multiplication to matrix multiplication, we have this inequality.<br>
And the total time can be improved slightly to n^2.78</p>
